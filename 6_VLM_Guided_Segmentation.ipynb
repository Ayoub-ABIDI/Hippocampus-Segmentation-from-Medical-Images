{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ef556b",
   "metadata": {},
   "source": [
    "VLM-GUIDED SEGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fec663",
   "metadata": {},
   "source": [
    "APPROCHES Ã€ TESTER:\n",
    "1. CLIP-guided U-Net: Features CLIP + U-Net\n",
    "2. Text-prompted segmentation: Descriptions textuelles â†’ masques\n",
    "3. Visual-Semantic fusion: Combiner embeddings visuels + textuels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b32c6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CLIP disponible!\n",
      "ğŸ–¥ï¸ Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.ndimage as ndi\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Pour CLIP (Vision-Language Model)\n",
    "try:\n",
    "    import clip\n",
    "    CLIP_AVAILABLE = True\n",
    "    print(\" CLIP disponible!\")\n",
    "except:\n",
    "    CLIP_AVAILABLE = False\n",
    "    print(\" CLIP non disponible. Installation: pip install git+https://github.com/openai/CLIP.git\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ–¥ï¸ Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aefbc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration prÃªte!\n",
      "ğŸ“ RÃ©sultats VLM: C:\\Users\\ayoub\\Downloads\\Task04_Hippocampus\\Task04_Hippocampus\\results\\vlm_segmentation\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = r\"C:\\Users\\ayoub\\Downloads\\Task04_Hippocampus\\Task04_Hippocampus\"  \n",
    "PREPROCESSED_PATH = os.path.join(DATASET_PATH, \"preprocessed\")\n",
    "RESULTS_PATH = os.path.join(DATASET_PATH, \"results\")\n",
    "VLM_PATH = os.path.join(RESULTS_PATH, \"vlm_segmentation\")\n",
    "os.makedirs(VLM_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"âœ… Configuration prÃªte!\")\n",
    "print(f\"ğŸ“ RÃ©sultats VLM: {VLM_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0fce65",
   "metadata": {},
   "source": [
    "Installation et chargement de CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8574390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338M/338M [01:09<00:00, 5.10MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CLIP chargÃ©!\n",
      "  Architecture: ViT-B/32\n",
      "  Image encoder: Vision Transformer\n",
      "  Text encoder: Transformer\n",
      "\n",
      "ğŸ§ª Test CLIP:\n",
      "  Text embeddings shape: torch.Size([3, 512])\n",
      "  Dimension: 512 (standard CLIP)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CLIP: Contrastive Language-Image Pre-training\n",
    "ModÃ¨le prÃ©-entraÃ®nÃ© sur 400M paires (image, texte)\n",
    "\"\"\"\n",
    "if CLIP_AVAILABLE:\n",
    "    # Charger CLIP prÃ©-entraÃ®nÃ©\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    clip_model.eval()\n",
    "    \n",
    "    print(\"âœ… CLIP chargÃ©!\")\n",
    "    print(f\"  Architecture: ViT-B/32\")\n",
    "    print(f\"  Image encoder: Vision Transformer\")\n",
    "    print(f\"  Text encoder: Transformer\")\n",
    "    \n",
    "    # Test CLIP\n",
    "    test_texts = [\n",
    "        \"anterior hippocampus in brain MRI\",\n",
    "        \"posterior hippocampus in brain scan\",\n",
    "        \"background tissue in medical image\"\n",
    "    ]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_tokens = clip.tokenize(test_texts).to(device)\n",
    "        text_features = clip_model.encode_text(text_tokens)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    print(f\"\\nğŸ§ª Test CLIP:\")\n",
    "    print(f\"  Text embeddings shape: {text_features.shape}\")\n",
    "    print(f\"  Dimension: 512 (standard CLIP)\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ CLIP non disponible. Utiliser une approche alternative.\")\n",
    "    print(\"   Installation: pip install git+https://github.com/openai/CLIP.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5e2d9",
   "metadata": {},
   "source": [
    "Descriptions textuelles anatomiques :\n",
    "\n",
    "Ces descriptions guident le modÃ¨le pour comprendre ce qu'il cherche.\n",
    "Plus les descriptions sont prÃ©cises, meilleure sera la segmentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5742f111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ DESCRIPTIONS TEXTUELLES DÃ‰FINIES:\n",
      "  Background: 5 descriptions\n",
      "  Anterior: 5 descriptions\n",
      "  Posterior: 5 descriptions\n",
      "  Context: 5 descriptions\n",
      "\n",
      "ğŸ’¡ EXEMPLE:\n",
      "  Anterior: 'anterior hippocampus in temporal lobe'\n",
      "  Posterior: 'posterior hippocampus in brain'\n"
     ]
    }
   ],
   "source": [
    "# Descriptions par classe\n",
    "CLASS_DESCRIPTIONS = {\n",
    "    'background': [\n",
    "        \"background tissue in brain MRI\",\n",
    "        \"non-hippocampus brain tissue\",\n",
    "        \"surrounding brain structures\",\n",
    "        \"white matter and gray matter\",\n",
    "        \"ventricles and CSF in brain\"\n",
    "    ],\n",
    "    'anterior': [\n",
    "        \"anterior hippocampus in temporal lobe\",\n",
    "        \"head of hippocampus, curved structure\",\n",
    "        \"anterior part of hippocampal formation\",\n",
    "        \"rostral hippocampus near amygdala\",\n",
    "        \"hippocampal head in coronal MRI slice\"\n",
    "    ],\n",
    "    'posterior': [\n",
    "        \"posterior hippocampus in brain\",\n",
    "        \"tail of hippocampus, elongated structure\",\n",
    "        \"posterior part of hippocampal formation\",\n",
    "        \"caudal hippocampus extending posteriorly\",\n",
    "        \"hippocampal tail in sagittal view\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Descriptions gÃ©nÃ©rales pour contexte global\n",
    "CONTEXT_DESCRIPTIONS = [\n",
    "    \"medical brain MRI scan showing temporal lobe structures\",\n",
    "    \"T1-weighted MRI of human brain with visible hippocampus\",\n",
    "    \"coronal brain slice with bilateral hippocampi\",\n",
    "    \"neuroanatomical structures in medial temporal lobe\",\n",
    "    \"high-resolution brain imaging of limbic system\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ DESCRIPTIONS TEXTUELLES DÃ‰FINIES:\")\n",
    "print(f\"  Background: {len(CLASS_DESCRIPTIONS['background'])} descriptions\")\n",
    "print(f\"  Anterior: {len(CLASS_DESCRIPTIONS['anterior'])} descriptions\")\n",
    "print(f\"  Posterior: {len(CLASS_DESCRIPTIONS['posterior'])} descriptions\")\n",
    "print(f\"  Context: {len(CONTEXT_DESCRIPTIONS)} descriptions\")\n",
    "\n",
    "print(\"\\nğŸ’¡ EXEMPLE:\")\n",
    "print(f\"  Anterior: '{CLASS_DESCRIPTIONS['anterior'][0]}'\")\n",
    "print(f\"  Posterior: '{CLASS_DESCRIPTIONS['posterior'][0]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacdcb3c",
   "metadata": {},
   "source": [
    "Encoder CLIP pour embeddings textuels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "228dc791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embeddings textuels gÃ©nÃ©rÃ©s!\n",
      "  Shape: torch.Size([3, 512])\n",
      "  Classes: Background, Anterior, Posterior\n",
      "\n",
      "ğŸ“Š SimilaritÃ©s entre classes:\n",
      "  Background â†” Anterior: 0.906\n",
      "  Background â†” Posterior: 0.903\n",
      "  Anterior â†” Posterior: 0.977\n",
      "  â†’ Plus la similaritÃ© est faible, mieux les classes sont sÃ©parÃ©es!\n"
     ]
    }
   ],
   "source": [
    "if CLIP_AVAILABLE:\n",
    "    def get_text_embeddings(descriptions_dict, clip_model, device):\n",
    "        \"\"\"\n",
    "        GÃ©nÃ¨re les embeddings textuels moyens pour chaque classe\n",
    "        \n",
    "        Returns:\n",
    "            text_embeddings: Tensor (num_classes, embedding_dim)\n",
    "        \"\"\"\n",
    "        embeddings_per_class = []\n",
    "        \n",
    "        for class_name in ['background', 'anterior', 'posterior']:\n",
    "            descriptions = descriptions_dict[class_name]\n",
    "            \n",
    "            # Tokenize et encoder\n",
    "            tokens = clip.tokenize(descriptions).to(device)\n",
    "            with torch.no_grad():\n",
    "                class_embeddings = clip_model.encode_text(tokens)\n",
    "                class_embeddings = class_embeddings / class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Moyenne des embeddings\n",
    "            mean_embedding = class_embeddings.mean(dim=0)\n",
    "            mean_embedding = mean_embedding / mean_embedding.norm()\n",
    "            \n",
    "            embeddings_per_class.append(mean_embedding)\n",
    "        \n",
    "        return torch.stack(embeddings_per_class)  # (3, 512)\n",
    "    \n",
    "    \n",
    "    # GÃ©nÃ©rer les embeddings\n",
    "    text_embeddings = get_text_embeddings(CLASS_DESCRIPTIONS, clip_model, device)\n",
    "    \n",
    "    print(f\"âœ… Embeddings textuels gÃ©nÃ©rÃ©s!\")\n",
    "    print(f\"  Shape: {text_embeddings.shape}\")\n",
    "    print(f\"  Classes: Background, Anterior, Posterior\")\n",
    "    \n",
    "    # Calculer similaritÃ© entre classes (doit Ãªtre faible)\n",
    "    similarities = torch.mm(text_embeddings, text_embeddings.t())\n",
    "    print(f\"\\nğŸ“Š SimilaritÃ©s entre classes:\")\n",
    "    print(f\"  Background â†” Anterior: {similarities[0, 1].item():.3f}\")\n",
    "    print(f\"  Background â†” Posterior: {similarities[0, 2].item():.3f}\")\n",
    "    print(f\"  Anterior â†” Posterior: {similarities[1, 2].item():.3f}\")\n",
    "    print(f\"  â†’ Plus la similaritÃ© est faible, mieux les classes sont sÃ©parÃ©es!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe9947f",
   "metadata": {},
   "source": [
    "Dataset avec preprocessing CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83493f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train: 13312 slices 2D (VLM dataset)\n",
      "  val: 3328 slices 2D (VLM dataset)\n",
      "\n",
      "âœ… DataLoaders VLM crÃ©Ã©s\n",
      "\n",
      "ğŸ§ª Test batch:\n",
      "  image_unet: torch.Size([8, 1, 64, 64])\n",
      "  image_clip: torch.Size([8, 3, 224, 224])\n",
      "  label: torch.Size([8, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "class HippocampusDatasetVLM(Dataset):\n",
    "    def __init__(self, file_list, preprocessed_path, split='train', \n",
    "                 augment_config=None, clip_preprocess=None):\n",
    "        self.file_list = file_list\n",
    "        self.preprocessed_path = preprocessed_path\n",
    "        self.split = split\n",
    "        self.augment_config = augment_config or {}\n",
    "        self.clip_preprocess = clip_preprocess\n",
    "        \n",
    "        # Liste des slices 2D\n",
    "        self.slices = []\n",
    "        for file_info in file_list:\n",
    "            base_name = file_info['image'].replace('.nii.gz', '')\n",
    "            img_path = os.path.join(preprocessed_path, split, f\"{base_name}_img.npy\")\n",
    "            lbl_path = os.path.join(preprocessed_path, split, f\"{base_name}_lbl.npy\")\n",
    "            \n",
    "            if os.path.exists(img_path):\n",
    "                volume = np.load(img_path)\n",
    "                num_slices = volume.shape[2]\n",
    "                \n",
    "                for slice_idx in range(num_slices):\n",
    "                    self.slices.append({\n",
    "                        'volume_path': img_path,\n",
    "                        'label_path': lbl_path,\n",
    "                        'slice_idx': slice_idx\n",
    "                    })\n",
    "        \n",
    "        print(f\"  {split}: {len(self.slices)} slices 2D (VLM dataset)\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.slices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        slice_info = self.slices[idx]\n",
    "        \n",
    "        volume = np.load(slice_info['volume_path'])\n",
    "        label = np.load(slice_info['label_path'])\n",
    "        \n",
    "        slice_idx = slice_info['slice_idx']\n",
    "        image_2d = volume[:, :, slice_idx]\n",
    "        label_2d = label[:, :, slice_idx]\n",
    "        \n",
    "        # Augmentation\n",
    "        if self.augment_config:\n",
    "            if self.augment_config.get('flip', False) and np.random.rand() > 0.5:\n",
    "                image_2d = np.flip(image_2d, axis=0).copy()\n",
    "                label_2d = np.flip(label_2d, axis=0).copy()\n",
    "            \n",
    "            if self.augment_config.get('rotation', False) and np.random.rand() > 0.7:\n",
    "                angle = np.random.uniform(-10, 10)\n",
    "                image_2d = ndi.rotate(image_2d, angle, reshape=False, order=1)\n",
    "                label_2d = ndi.rotate(label_2d, angle, reshape=False, order=0)\n",
    "        \n",
    "        # Format standard pour U-Net\n",
    "        image_unet = torch.from_numpy(image_2d).float().unsqueeze(0)  # (1, H, W)\n",
    "        \n",
    "        # Format CLIP (si disponible)\n",
    "        if self.clip_preprocess is not None and CLIP_AVAILABLE:\n",
    "            # Convertir en RGB (3 canaux) pour CLIP\n",
    "            image_rgb = np.stack([image_2d]*3, axis=-1)\n",
    "            # Normaliser [0, 1]\n",
    "            image_rgb = (image_rgb - image_rgb.min()) / (image_rgb.max() - image_rgb.min() + 1e-8)\n",
    "            # Convertir en PIL puis appliquer preprocessing CLIP\n",
    "            from PIL import Image\n",
    "            image_pil = Image.fromarray((image_rgb * 255).astype(np.uint8))\n",
    "            image_clip = self.clip_preprocess(image_pil)\n",
    "        else:\n",
    "            image_clip = None\n",
    "        \n",
    "        label_2d = torch.from_numpy(label_2d).long()\n",
    "        \n",
    "        return {\n",
    "            'image_unet': image_unet,\n",
    "            'image_clip': image_clip,\n",
    "            'label': label_2d\n",
    "        }\n",
    "\n",
    "\n",
    "# Charger donnÃ©es\n",
    "with open(os.path.join(PREPROCESSED_PATH, 'split_info.json'), 'r') as f:\n",
    "    split_info = json.load(f)\n",
    "\n",
    "train_files = split_info['train']\n",
    "val_files = split_info['val']\n",
    "\n",
    "augment_config = {'flip': True, 'rotation': True}\n",
    "\n",
    "if CLIP_AVAILABLE:\n",
    "    train_dataset_vlm = HippocampusDatasetVLM(\n",
    "        train_files, PREPROCESSED_PATH, split='train',\n",
    "        augment_config=augment_config, clip_preprocess=preprocess\n",
    "    )\n",
    "    \n",
    "    val_dataset_vlm = HippocampusDatasetVLM(\n",
    "        val_files, PREPROCESSED_PATH, split='val',\n",
    "        augment_config=None, clip_preprocess=preprocess\n",
    "    )\n",
    "    \n",
    "    train_loader_vlm = DataLoader(train_dataset_vlm, batch_size=8, shuffle=True, num_workers=0)\n",
    "    val_loader_vlm = DataLoader(val_dataset_vlm, batch_size=8, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"\\nâœ… DataLoaders VLM crÃ©Ã©s\")\n",
    "    \n",
    "    # Test\n",
    "    batch = next(iter(train_loader_vlm))\n",
    "    print(f\"\\nğŸ§ª Test batch:\")\n",
    "    print(f\"  image_unet: {batch['image_unet'].shape}\")\n",
    "    print(f\"  image_clip: {batch['image_clip'].shape if batch['image_clip'] is not None else None}\")\n",
    "    print(f\"  label: {batch['label'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b8f74",
   "metadata": {},
   "source": [
    "CLIP-Guided U-Net - Architecture hybride"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7d2af3",
   "metadata": {},
   "source": [
    "Architecture innovante qui combine:\n",
    "- U-Net pour segmentation\n",
    "- CLIP features pour guidance sÃ©mantique\n",
    "- Fusion des embeddings visuels et textuels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9f02b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CLIPGuidedUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    CLIP-Guided U-Net (Corrected Version)\n",
    "\n",
    "    - CLIP visual encoder (frozen)\n",
    "    - U-Net for spatial precision\n",
    "    - Visual-Semantic alignment via cosine similarity (CLIP-style)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clip_model, text_embeddings, num_classes=3, base_filters=32):\n",
    "        super().__init__()\n",
    "\n",
    "        # -----------------------------\n",
    "        # CLIP VISUAL ENCODER (FROZEN)\n",
    "        # -----------------------------\n",
    "        self.clip_visual = clip_model.visual\n",
    "        for p in self.clip_visual.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.register_buffer(\"text_embeddings\", text_embeddings)  # (C, 512)\n",
    "\n",
    "        # Project CLIP â†’ bottleneck\n",
    "        self.clip_projection = nn.Sequential(\n",
    "            nn.Linear(512, base_filters * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(base_filters * 8, base_filters * 8)\n",
    "        )\n",
    "\n",
    "        # -----------------------------\n",
    "        # U-NET ENCODER\n",
    "        # -----------------------------\n",
    "        self.enc1 = self.conv_block(1, base_filters)\n",
    "        self.enc2 = self.conv_block(base_filters, base_filters * 2)\n",
    "        self.enc3 = self.conv_block(base_filters * 2, base_filters * 4)\n",
    "        self.enc4 = self.conv_block(base_filters * 4, base_filters * 8)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # -----------------------------\n",
    "        # BOTTLENECK + FUSION\n",
    "        # -----------------------------\n",
    "        self.bottleneck = self.conv_block(base_filters * 8, base_filters * 16)\n",
    "\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(base_filters * 16 + base_filters * 8, base_filters * 16, 1),\n",
    "            nn.BatchNorm2d(base_filters * 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_filters * 16, base_filters * 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_filters * 16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # -----------------------------\n",
    "        # U-NET DECODER\n",
    "        # -----------------------------\n",
    "        self.up4 = nn.ConvTranspose2d(base_filters * 16, base_filters * 8, 2, 2)\n",
    "        self.dec4 = self.conv_block(base_filters * 16, base_filters * 8)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(base_filters * 8, base_filters * 4, 2, 2)\n",
    "        self.dec3 = self.conv_block(base_filters * 8, base_filters * 4)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(base_filters * 4, base_filters * 2, 2, 2)\n",
    "        self.dec2 = self.conv_block(base_filters * 4, base_filters * 2)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(base_filters * 2, base_filters, 2, 2)\n",
    "        self.dec1 = self.conv_block(base_filters * 2, base_filters)\n",
    "\n",
    "        # -----------------------------\n",
    "        # CLIP-STYLE PIXEL CLASSIFIER\n",
    "        # -----------------------------\n",
    "        self.visual_proj = nn.Conv2d(base_filters, 512, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_unet, x_clip=None):\n",
    "        \"\"\"\n",
    "        x_unet: (B, 1, H, W)\n",
    "        x_clip: (B, 3, 224, 224) or None\n",
    "        \"\"\"\n",
    "\n",
    "        B, _, H, W = x_unet.shape\n",
    "\n",
    "        # -----------------------------\n",
    "        # CLIP FEATURES\n",
    "        # -----------------------------\n",
    "        if x_clip is not None:\n",
    "            with torch.no_grad():\n",
    "                clip_feat = self.clip_visual(x_clip)  # (B, 512)\n",
    "            clip_feat = self.clip_projection(clip_feat)\n",
    "            clip_feat = clip_feat.view(B, -1, 1, 1)\n",
    "        else:\n",
    "            clip_feat = torch.zeros(\n",
    "                B, self.enc4[0].out_channels, 1, 1,\n",
    "                device=x_unet.device\n",
    "            )\n",
    "\n",
    "        # -----------------------------\n",
    "        # ENCODER\n",
    "        # -----------------------------\n",
    "        e1 = self.enc1(x_unet)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "\n",
    "        # -----------------------------\n",
    "        # BOTTLENECK + FUSION\n",
    "        # -----------------------------\n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "\n",
    "        clip_feat = clip_feat.expand(-1, -1, b.size(2), b.size(3))\n",
    "        b = self.fusion(torch.cat([b, clip_feat], dim=1))\n",
    "\n",
    "        # -----------------------------\n",
    "        # DECODER\n",
    "        # -----------------------------\n",
    "        d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1))\n",
    "        d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "\n",
    "        # -----------------------------\n",
    "        # CLIP-STYLE SEGMENTATION\n",
    "        # -----------------------------\n",
    "        visual = self.visual_proj(d1)           # (B, 512, H, W)\n",
    "        visual = F.normalize(visual, dim=1)\n",
    "\n",
    "        text = F.normalize(self.text_embeddings, dim=1)  # (C, 512)\n",
    "\n",
    "        logits = torch.einsum(\"bchw,nc->bnhw\", visual, text)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75768eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "model_vlm = CLIPGuidedUNet(\n",
    "    clip_model=clip_model,\n",
    "    text_embeddings=text_embeddings,\n",
    "    num_classes=3\n",
    ").to(device)\n",
    "\n",
    "x_unet = torch.randn(2, 1, 64, 64).to(device)\n",
    "x_clip = torch.randn(2, 3, 224, 224).to(device)\n",
    "\n",
    "y = model_vlm(x_unet, x_clip)\n",
    "\n",
    "print(\"Output shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341cefcd",
   "metadata": {},
   "source": [
    "Loss Functions et mÃ©triques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28a393fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loss functions et mÃ©triques VLM prÃªtes\n"
     ]
    }
   ],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.softmax(pred, dim=1)\n",
    "        target_one_hot = torch.nn.functional.one_hot(target, num_classes=pred.shape[1])\n",
    "        target_one_hot = target_one_hot.permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        intersection = (pred * target_one_hot).sum(dim=(2, 3))\n",
    "        union = pred.sum(dim=(2, 3)) + target_one_hot.sum(dim=(2, 3))\n",
    "        \n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "\n",
    "class VLMGuidedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss combinÃ©e pour VLM:\n",
    "    - Dice Loss pour segmentation\n",
    "    - Consistency loss pour alignement vision-texte\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_dice=0.8, weight_consistency=0.2):\n",
    "        super(VLMGuidedLoss, self).__init__()\n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.weight_dice = weight_dice\n",
    "        self.weight_consistency = weight_consistency\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        dice = self.dice_loss(pred, target)\n",
    "        ce = self.ce_loss(pred, target)\n",
    "        \n",
    "        # Consistency: encourage semantic coherence\n",
    "        total_loss = self.weight_dice * dice + self.weight_consistency * ce\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "\n",
    "def dice_score(pred, target, num_classes=3):\n",
    "    pred = torch.softmax(pred, dim=1)\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    \n",
    "    dice_scores = []\n",
    "    for c in range(num_classes):\n",
    "        pred_c = (pred == c).float()\n",
    "        target_c = (target == c).float()\n",
    "        \n",
    "        intersection = (pred_c * target_c).sum()\n",
    "        union = pred_c.sum() + target_c.sum()\n",
    "        \n",
    "        if union == 0:\n",
    "            dice_scores.append(1.0)\n",
    "        else:\n",
    "            dice_scores.append((2.0 * intersection / union).item())\n",
    "    \n",
    "    return dice_scores\n",
    "\n",
    "\n",
    "def train_epoch_vlm(model, loader, criterion, optimizer, device, use_clip=True):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Training VLM', leave=False):\n",
    "        images_unet = batch['image_unet'].to(device)\n",
    "        images_clip = batch['image_clip'].to(device) if use_clip and batch['image_clip'] is not None else None\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images_unet, images_clip)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate_vlm(model, loader, criterion, device, use_clip=True):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    dice_scores_all = defaultdict(list)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Validation VLM', leave=False):\n",
    "            images_unet = batch['image_unet'].to(device)\n",
    "            images_clip = batch['image_clip'].to(device) if use_clip and batch['image_clip'] is not None else None\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(images_unet, images_clip)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            scores = dice_score(outputs, labels)\n",
    "            for i, score in enumerate(scores):\n",
    "                dice_scores_all[f'class_{i}'].append(score)\n",
    "    \n",
    "    avg_dice = {k: np.mean(v) for k, v in dice_scores_all.items()}\n",
    "    \n",
    "    return total_loss / len(loader), avg_dice\n",
    "\n",
    "\n",
    "print(\"âœ… Loss functions et mÃ©triques VLM prÃªtes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bece24",
   "metadata": {},
   "source": [
    "EXPÃ‰RIENCE VLM-1 - CLIP-Guided U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94656770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€\n",
      "EXPÃ‰RIENCE VLM-1: CLIP-GUIDED U-NET\n",
      "ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€\n",
      "\n",
      "ğŸ“Š Training VLM-1 for 10 epochs\n",
      "ğŸ”’ CLIP encoder: FROZEN\n",
      "ğŸ”“ U-Net + Fusion layers: TRAINABLE\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.8099 | Val Loss: 0.8001 | Avg Dice: 0.7763\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.7996 | Val Loss: 0.7992 | Avg Dice: 0.7483\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.7993 | Val Loss: 0.7991 | Avg Dice: 0.8144\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.7990 | Val Loss: 0.7988 | Avg Dice: 0.9174\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.7988 | Val Loss: 0.7988 | Avg Dice: 0.8932\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.7988 | Val Loss: 0.7987 | Avg Dice: 0.9155\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.7988 | Val Loss: 0.7987 | Avg Dice: 0.8889\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.7988 | Val Loss: 0.7987 | Avg Dice: 0.8964\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.7988 | Val Loss: 0.7987 | Avg Dice: 0.9145\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.7988 | Val Loss: 0.7987 | Avg Dice: 0.9084\n",
      "\n",
      "âœ… VLM-1 terminÃ©\n",
      "ğŸ¯ Final Dice Score (CLIP-Guided): 0.9084\n"
     ]
    }
   ],
   "source": [
    "if CLIP_AVAILABLE:\n",
    "    print(\"\\n\" + \"ğŸš€\" * 35)\n",
    "    print(\"EXPÃ‰RIENCE VLM-1: CLIP-GUIDED U-NET\")\n",
    "    print(\"ğŸš€\" * 35)\n",
    "\n",
    "    # =============================\n",
    "    # CONFIGURATION\n",
    "    # =============================\n",
    "    NUM_EPOCHS_VLM = 10\n",
    "    LEARNING_RATE_VLM = 1e-4\n",
    "    WEIGHT_DECAY_VLM = 1e-5\n",
    "\n",
    "    exp_vlm1_config = {\n",
    "        'name': 'VLM1_CLIPGuidedUNet',\n",
    "        'architecture': 'CLIP-Guided U-Net',\n",
    "        'clip_model': 'ViT-B/32 (frozen)',\n",
    "        'augmentation': 'Flip + Rotation',\n",
    "        'loss': 'VLMGuidedLoss',\n",
    "        'base_filters': 32\n",
    "    }\n",
    "\n",
    "    # =============================\n",
    "    # MODEL\n",
    "    # =============================\n",
    "    model_vlm1 = CLIPGuidedUNet(\n",
    "        clip_model=clip_model,\n",
    "        text_embeddings=text_embeddings,\n",
    "        num_classes=3,\n",
    "        base_filters=32\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss\n",
    "    criterion_vlm1 = VLMGuidedLoss(\n",
    "        weight_dice=0.8,\n",
    "        weight_consistency=0.2\n",
    "    )\n",
    "\n",
    "    # Optimizer (CLIP frozen)\n",
    "    trainable_params = [p for p in model_vlm1.parameters() if p.requires_grad]\n",
    "    optimizer_vlm1 = optim.Adam(\n",
    "        trainable_params,\n",
    "        lr=LEARNING_RATE_VLM,\n",
    "        weight_decay=WEIGHT_DECAY_VLM\n",
    "    )\n",
    "\n",
    "    # =============================\n",
    "    # HISTORY\n",
    "    # =============================\n",
    "    history_vlm1 = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_dice': []\n",
    "    }\n",
    "\n",
    "    print(f\"\\nğŸ“Š Training VLM-1 for {NUM_EPOCHS_VLM} epochs\")\n",
    "    print(\"ğŸ”’ CLIP encoder: FROZEN\")\n",
    "    print(\"ğŸ”“ U-Net + Fusion layers: TRAINABLE\")\n",
    "\n",
    "    # =============================\n",
    "    # TRAINING LOOP\n",
    "    # =============================\n",
    "    for epoch in range(NUM_EPOCHS_VLM):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS_VLM}\")\n",
    "\n",
    "        # ---- TRAIN (WITH CLIP) ----\n",
    "        train_loss = train_epoch_vlm(\n",
    "            model_vlm1,\n",
    "            train_loader_vlm,\n",
    "            criterion_vlm1,\n",
    "            optimizer_vlm1,\n",
    "            device,\n",
    "            use_clip=True\n",
    "        )\n",
    "        history_vlm1['train_loss'].append(train_loss)\n",
    "\n",
    "        # ---- VALIDATION (WITH CLIP) ----\n",
    "        val_loss, val_dice = validate_vlm(\n",
    "            model_vlm1,\n",
    "            val_loader_vlm,\n",
    "            criterion_vlm1,\n",
    "            device,\n",
    "            use_clip=True\n",
    "        )\n",
    "        history_vlm1['val_loss'].append(val_loss)\n",
    "        history_vlm1['val_dice'].append(val_dice)\n",
    "\n",
    "        avg_dice = np.mean(list(val_dice.values()))\n",
    "        print(\n",
    "            f\"  Train Loss: {train_loss:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} | \"\n",
    "            f\"Avg Dice: {avg_dice:.4f}\"\n",
    "        )\n",
    "\n",
    "    # =============================\n",
    "    # SAVE MODEL\n",
    "    # =============================\n",
    "    torch.save(\n",
    "        model_vlm1.state_dict(),\n",
    "        os.path.join(VLM_PATH, \"vlm1_clip_guided_unet.pth\")\n",
    "    )\n",
    "\n",
    "    final_dice_vlm1 = avg_dice\n",
    "    print(f\"\\nâœ… VLM-1 terminÃ©\")\n",
    "    print(f\"ğŸ¯ Final Dice Score (CLIP-Guided): {final_dice_vlm1:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ CLIP non disponible â€” expÃ©rience VLM-1 ignorÃ©e\")\n",
    "    final_dice_vlm1 = None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
